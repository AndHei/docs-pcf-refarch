---
title: Reference Architectures for Pivotal Cloud Foundry on vSphere
owner: Customer0
---

<strong><%= modified_date %></strong>

This guide presents reference architectures for Pivotal Cloud Foundry (PCF) on vSphere.

Pivotal validates the reference architectures described in this topic against multiple production-grade usage scenarios. These test deployments host up to 1500 app instances and use PCF-managed services such as MySQL, RabbitMQ, and Spring Cloud Services.

This document does not replace the [basic installation documentation] (http://docs.pivotal.io/pivotalcf/1-8/customizing/vsphere.html), but gives proven examples of how to apply those instructions to real-world production environments.

| PCF Products Validated        | Version                   |
| ----------------------------- |:-------------------------:|
| PCF Ops Manager               | 1.8.latest                |
| Elastic Runtime               | 1.8.latest                |

## <a id="arch_diagram"></a> Base Reference Architecture 

This diagram shows an architecture for two PCF installations sharing the same vSphere server clusters, yet segmented from each other with Resource Pools. This design supports long term use and growth, while allowing for capacity growth at the vSphere level and also maximum installation security. It allocates 3+ servers to each cluster, as recommended for vSphere, and spreads PCF components across 3 (or another odd number) of clusters, as [recommended](../../concepts/high-availability.html) for PCF.

This recommended architecture relies on VMware NSX SDN (software-defined networking). See below for architectures that do not use NSX.

<%= image_tag('vsphere-overview-arch.png') %>

To view a larger version of this diagram, click [here](../images/vsphere-overview-arch.png).

### <a id="install"></a> Installation

To create a system following this architecture, you do the following:

1. From vCenter, create three clusters.

1. Populate each cluster with two VMware Resource Pools. Enable VMware distributed resource scheduler (DRS) for each Resource Pool, so vMotion can automatically migrate data to avoid downtime.

1. For hosting capacity, populate each cluster with three ESXi hosts, making nine hosts for each installation. All installations collectively draw from the same nine hosts.

1. In one PCF deployment, use Ops Manager to create three Availability Zones (AZs), each corresponding to one of the Resource Pools from each cluster.

1. In the other PCF deployment, create an AZ for each of the three remaining Resource Pools.

1. For storage, add dedicated datastores to each PCF deployment following one of the two approaches described [below](#storage).

1. Supply core networking for each deployment by configuring an NSX Edge with the following subnets. See [below](#networking) for details:
  - Infrastructure
  - Elastic Runtime (ERT)
  - Service tile(s)

### <a id="scale"></a> Scaling

You can easily scale up this architecture to support additional PCF installations  with the same capacity with the assurance that each is resource-protected and separated.

To support more PCF installations, scale this architecture vertically by adding Resource Pools. To add capacity to all PCF installations, scale it horizontally by adding hosts to the existing clusters in sets of three, one per cluster. 

### <a id="priority"></a> Priority

In this architecture, multiple PCF installations share host resources. You can use vCenter resource allocation shares to assign `High`, `Normal`, or `Low` priority to pools used by different installations. When host resources keep up demand, these share value makes no difference, but when multiple installations compete for limited resources, you can prioritize a production installation over a development installation (for example) by assigning its resource pools a `High` share value setting.

### <a id="storage"></a> Storage Configuration

You can allocate networked storage to the host clusters following one of two common approaches, _horizontal_ or _vertical_:

1. In the horizontal approach, you grant all hosts access to all datastores, and assign a subset to each installation. With 6 datastores `ds01` through `ds06`, for example, you grant all nine hosts access to all six datastores, then provision PCF installation #1 to use stores `ds01` through `ds03`, and installation #2 to use `ds04` through `ds06`. Installation #1 will use `ds01` until it is full, then `ds02`, and so on.

2. In the vertical approach, which is how vSphere VSAN works, you grant each host cluster its own dedicated datastores. This gives each installation multiple datastores to provide storage for VMs based on their host cluster. With 6 datastores `ds01` through `ds06`, for example, you assign datastores `ds01` and `ds02` to Cluster 1, `ds03` and `ds04` to Cluster 2, and `ds05` and `ds06` to Cluster 3. Then you provision PCF installation #1 to use `ds01`, `ds03` and `ds05`, and installation #2 to use `ds02`, `ds04` and `ds06`. With this arrangement, all VMs in the same installation and cluster share a dedicated datastore.

<p class="note"><strong>Note:</strong> If a vSphere datastore is part of a vSphere Storage Cluster using sDRS (storage DRS), the sDRS feature must be disabled on the datastores used by PCF as s-vMotion activity will cause BOSH to malfunction as a result of renaming managed independent disks.</p>

### <a id="storage-type"></a> Storage Capacity and Type

Pivotal recommends allocating at least 16TB of data storage for a typical PCF installation, either as two 8TB stores or a greater number of smaller volumes. Small installations without many tiles can use less; two 4TB volumes is reasonable.

For storage types, Pivotal recommends block-based (fiber channel or iSCSI) and file-based (NFS) over high-speed carriers such as 6G FC or 10GigE. Redundant storage is highly recommended for any persistent data, but you can use DASD or JBOD for ephemeral data.

### <a id="networking"></a> Networking

Using VMware NSX SDN (software-defined networking) in this architecture provides the following benefits:

  1. Firewall capability per-installation through the built-in Edge firewall
  2. High capacity, resilient load balancing per-installation through the NSX Load Balancer
  3. Installation obfuscation through the use of non-routed RFC networks behind the NSX Edge and the use of SNAT/DNAT connections to expose only the endpoints of Cloud Foundry that need exposure.
  4. High repeatability of installations through the repeat use of all network and addressing conventions on the right hand side of the diagram (the Tenant Side)
  5. Automatic rule and ACL sharing via NSX Manager Global Ruleset
  6. Automatic HA pairing of NSX Edges, managed by NSX Manager
  7. Support for PCF Go Router IP membership in the NSX Edge virtual load balancer pool by the BOSH CPI (not an Ops Manager feature)

This architecture does not use the following products:

* NSX DLR (Distributed Logical Router) only provides routing services, not load balancing and firewalling.

* NSX DLF (Distributed Logical Firewall) - NSX SDN already provides networking capability where it is most needed, in front of the PCF installation, not on the network(s) that the installation uses. It also eliminates the need for micro-segmentation, because NSX Edge with SDN eliminates the need to place firewall rules horizontally on a network used by PCF.

* NSX DLB (Distributed Load Balancing) is not considered production-quality and is not intended for use with L7 balancing (which is PCF's primary need) or for North/South flows.

#### Networking Design

Each PCF installation consumes three (or more) networks from the NSX Edge, aligned to specific job types:

- **Infrastructure**: This "inward-facing" network has a small CIDR range and includes resources that interact with the IaaS layer and back-office systems, such as the cloud provider interface (CPI), Ops Manager, BOSH, and other utility VMs such as jumpbox VM.
- **Deployment**: This network has a large CIDR range. Elastic Runtime uses it exclusively, to deploy app containers and related support components. Also known as "the apps wire."
- **Services**: This network (or multiple networks) has a large CIDR range. Services installed with Ops Manager tiles and managed by BOSH . A simple approach is to use this network for all PCF tiles except ERT. A more involved approach would be to deploy multiple "Services-#" networks, one for each tile or one for each type of tile, say databases vs message busses and so on.

All of these networks are considered "inside" or "tenant-side" networks, and use non-routable RFC network space to make provisioning repeatable. The NSX Edge translates between the tenant and service provider side networks using SNAT and DNAT.

Each NSX Edge should be provisioned with at least four service provider side (routable) IPs:

1. A static IP by which NSX Manager will manage the NSX Edge
2. A static IP for use as egress SNAT (traffic from tenant side will exit the Edge on this IP)
3. A static IP for DNATs to Ops Manager
4. A static IP for the load balancer VIP that will balance to a pool of PCF Go Routers

  _There are many more uses for IPs on the routed side of the Edge. Ten reserved, contiguous static IPs are recommended per NSX Edge for flexibility and future needs._

On the tenant side, each interface on the Edge that is defines will act as the IP gateway for the network used on that port group. The following are recommend for use on these networks:
- "Infra" network: 192.168.10.0/26, Gateway at .1
- "Deployment" network: 192.168.20.0/22, Gateway at .1
- "Services" network: 192.168.24.0/22, Gateway at .1
- _"Services-B" network: 192.168.28.0/22, and so on..._

  <%= image_tag('vsphere-exploded-edge.png') %>

  To view a larger version of this diagram, click [here](../images/vsphere-exploded-edge.png).

  vSphere DVS (distributed virtual switching) is recommended for all Clusters used by PCF. NSX will create a DPG (distributed port group) for each interface provisioned on the NSX Edge. TK CROSS-REFERENCE FIGURES - each DPG (pcf0#-infra etc.) defines the network IPs, mask etc. shown in the figure above (“Infra” Network etc.) TK

  <%= image_tag('vsphere-port-groups.png') %>

  To view a larger version of this diagram, click [here](../images/vsphere-port-groups.png).

### Reference Approach Without VMware NSX

In the absence of VMware NSX SDN technology, the PCF installation on vSphere follows the standard approach discussed in the documentation. For the purposes of this reference architecture, it would be easist to explore what changes and/or is lost in this approach.

*__Networking Features__*

- Load balancing would have to be hosted by some external service, such as a hardware appliance or VM from a 3rd party. This also applies to SSL termination.
- Pre-installation firewalling would be lost, as the traditional approach to firewalling inside systems is per zone or per network, not per virtual appliance installation that spans multiple networks.
- The need to SNAT/DNAT non-routable RFC networks used with PCF would go away as it's unlikely they would be used at all without the NSX Edge there to provide the boundary. In it's place a single, or possible multiple VLANs from the routable network space already deployed in the datacenter would be used.

*__Networking Design__*

The more traditional approach without SDN would be to deploy a single VLAN for use with all of PCF, or possibly a pair of VLANs (one for infrastructure and one for PCF).

<%= image_tag('vsphere-no-nsx.png') %>

To view a larger version of this diagram, click [here](../images/vsphere-no-nsx.png).

In this example, the functions of firewall and load balancer have been moved outside the of vSphere space to generic devices assumed to be available in the datacenter. The PCF installation is now bound to two port groups provided by a DVS on ESXi, each one aligned to a key use case:

  1. "Infra": PCF VMs used to communicate w/the IaaS layer
  2. "PCF": the primary deployment network for all tiles including ERT

Each one of these port groups typically is assigned a VLAN out of the datacenter's pool and a routable IP address segment. Routing functions are handled by switching layers outside of vSphere, such as TOR or EOR switch/router.

### Reference Approach Without Three Clusters

Some desire to start with PCF aligned to less resource than the standard (above) calls for, so the starting point for that is a single Cluster. If you are working with at least three ESXi hosts, the recommended guidance is still to setup in three Clusters, even with one host in each (such that the HA comes fro the PasS, not the IaaS), but for less than that, place all available hosts into a single Cluster with DRS and HA enabled.

<%= image_tag('vsphere-single-cluster.png') %>

To view a larger version of this diagram, click [here](../images/vsphere-single-cluster.png).

A two Cluster configuration has little value compared to a single or triple cluster configuration. While a pair of Clusters has symmetry in vSphere, PCF always seeks to deploy resources in odd numbers, so a two Cluster configuration forces the operator into a two AZ alignment for odd (three) elements, which is far from ideal.

*__Network Design__*

It is recommended to use the networking approach detailed in either the with-NSX or without-NSX sections for this design, as the compute arrangement has little impact on how PCF in networked for production use.

*__Storage__*

It is recommended that all datastores to be used by PCF be mapped to all the hosts in the single cluster. Otherwise, follow the guidance from (above).

### Reference Approach Utilizing Multi-Datacenter

For some scenarios, deploying PCF across combined resources located in more than one site is desirable to avoid total loss of site. There are a number of approaches architects may take to solve this problem, each with it's own caveats.

TL;DR PCF Multi-Datacenter is a plausible approach that's flawed in one way or another depending on the architecture.

*__Multi-Datacenter vSphere With Stretched Clusters__*

In this approach, the architect is treating two sites as the same logical capacity and is building Clusters from components from both sites at the same time. Given four hosts, two might come from "East" and two might come from "West". In vSphere, these appear to form a four host Cluster. Networking is applied such that all hosts see the same networks thru stretched layer 2 application or perhaps a SDN solution such as NSX is being used to tunnel L2 over L3.

<%= image_tag('vsphere-multi-datacenter.png') %>

To view a larger version of this diagram, click [here](../images/vsphere-multi-datacenter.png).

In terms of PCF, the Cluster is an AZ and BOSH has no sense of some of that capacity coming from different places. Thus, these hosts must be able to operate such that there's no practical difference between the networks and storage they attach to, in terms of latency and connectivity.

To honor the (above) gold standard, the approach should be three Clusters in use, but drawing from two sites, yielding a 4x3x3 type of deployment.

- Four hosts per cluster (two from each site)
- Three clusters for PCF as AZs
- Three AZs mapped to Clusters in PCF

Also, the single cluster model (above) can be used. This may be the more practical approach, since so many resources from both sites are already being applied to achieve HA.

Replicated storage between sites is assumed. Datastores must be common to all hosts in a cluster for seamless operation, or else VMs will become trapped on the hosts mapped to specific datastores and won't vMotion away for maintenance or move for DRS.

An interesting strategy for this model to ensure high availability for PCF is to keep a record of how many hosts are in a cluster and deploy enough copies of a PCF job in that AZ to ensure survivability in a site loss. This means placing large, odd numbers of jobs (such as consul) in the cluster so that at least two are left on either site in the event of a loss of site. In a four host cluster, this would call for five consul job VMs, so each site has at least two if not the third. DRS anti-affinity rules can be used here (set at the IaaS level) to force like VMs apart for best effect.

Also, lots of smaller Diego Cells are recommended over a few, very large Diego Cells.

Network traffic is a challenge in this scenario, as app traffic may enter at any point in either site's connection points, but can only leave at a designated gateway. Thus, it's possible to have apps servicing traffic coming from either East or West but only appearing to respond via West (as in the diagram) causing a "trombone effect" of traffic doubling across datacenter links. The architect should consider the impact of hosting apps that may land in East only to have the traffic flow out of West.

*__Multi-Datacenter vSphere With Combined East/West Clusters__*

In this approach, the architect is drawing capacity from the two sites independently and offering clusters of this capacity to PCF in distinct sets. This could yield vSphere Clusters always in pairs (East & West), so honoring PCF's need to deploy in odd numbers can become problematic.

One strategy here would be to effectively double the standard approach at the beginning of this material, yielding six total clusters, three from each side. While this seems like a whole lot of gear to apply to PCF, you could argue that in a BC/DR type of scenario, doubling everything is exactly the point.

Another strategy is to use the Single Cluster approach from above, where you have three resource pools in one cluster per site, yielding six AZs to PCF but only using one actual cluster of capacity from each site. This approach won't scale as readily but does have the benefit of drawing capacity from only one cluster, which is east to provision with only a few hosts.

Storage replication in this case is less critical as the assumption is there are enough AZs from either side to survive a failure and vSphere HA isn't needed to recover the installation.
